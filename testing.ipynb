{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import *\n",
    "import torch\n",
    "from dataloaders import *\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pandas as pd\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomended Approach: USE parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dataset = SimpleParquetDataset(P_TRAIN_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCACAGCGCUGGGUUCGCCCAGCGCAAAAGAAACAACAACAACAAC',\n",
       " 'DATA/Ribonanza_bpp_files/extra_data/0/0/0/51e61fbde94d.txt')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_dataset.sequence_df.row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(parquet_dataset, [0.7, 0.3], generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)  \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SimpleParquetDataset(P_TEST_PARQUET, train_test_flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=8, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTTIONAL: use pandas \n",
    "uncomment if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(P_TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = CNNDataset(data_csv=P_TRAIN_CSV, target_csv=P_TARGETS_CSV, matrix_dir=ETERNA_PKG_BPP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.matrix_df[dataset.matrix_df['sequence_id'] == 'eee73c1836bc']['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator1 = torch.Generator().manual_seed(42)\n",
    "# train_dataset, val_dataset = random_split(parquet_dataset, [0.7, 0.3], generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)  \n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq id:  (1343823,)\n",
      "mamtrix:  (1343823, 2)\n",
      "Index(['Unnamed: 0', 'sequence_id', 'sequence', 'future', 'seq_len'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = CNNDataset(data_csv=P_TEST_CSV, train_test_flag='test', matrix_dir=ETERNA_PKG_BPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=8, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DATA\\\\Ribonanza_bpp_files\\\\extra_data\\\\6\\\\0\\\\0\\\\eee73c1836bc.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# test_dataset.matrix_df[test_dataset.matrix_df['sequence_id'] == 'eee73c1836bc']['path'].values[0]\n",
    "#test_dataset.matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# for x, y in train_dataloader:\n",
    "#     print(x.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am the seq id:  eee73c1836bc\n",
      "I am the path:  806897    DATA\\Ribonanza_bpp_files\\extra_data\\6\\0\\0\\eee7...\n",
      "Name: path, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# import torch.optim as optim\n",
    "# from torch.nn import CTCLoss\n",
    "\n",
    "# from dataloaders import CNNDataset\n",
    "# from config import *\n",
    "\n",
    "\n",
    "# # ðŸ“‰ Define loss functions for training and evaluation\n",
    "# def loss_fn(output, target):\n",
    "#     # ðŸªŸ Clip the target values to be within the range [0, 1]\n",
    "#     clipped_target = torch.clip(target, min=0, max=1)\n",
    "#     # ðŸ“‰ Calculate the mean squared error loss\n",
    "#     mses = F.l1_loss(output, clipped_target, reduction='mean')\n",
    "#     return mses\n",
    "\n",
    "# # def mae_fn(output, target):\n",
    "# #     # ðŸªŸ Clip the target values to be within the range [0, 1]\n",
    "# #     clipped_target = torch.clip(target, min=0, max=1)\n",
    "# #     # ðŸ“‰ Calculate the mean absolute error loss\n",
    "# #     maes = F.l1_loss(output, clipped_target, reduction='mean')\n",
    "# #     return maes\n",
    "\n",
    "# def train_batch(crnn, data, optimizer, criterion, device):\n",
    "#     #crnn.train()\n",
    "\n",
    "#     images, targets= [d.to(device) for d in data]\n",
    "\n",
    "#     logits = crnn(images)\n",
    "#     #log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "#     log_probs = logits\n",
    "\n",
    "#     #atch_size = images.size(0)\n",
    "#     #input_lengths = torch.LongTensor([logits.size(0)] * batch_size)\n",
    "#     #target_lengths = torch.flatten(input_lengths)\n",
    "\n",
    "#     loss = criterion(log_probs, targets)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(crnn.parameters(), 5) # gradient clipping with 5\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     epochs = 1\n",
    "#     #dataset = CNNDataset(data_csv=P_TRAIN_CSV, target_csv=P_TARGETS_CSV, matrix_dir=ETERNA_PKG_BPP )\n",
    "#     generator1 = torch.Generator().manual_seed(42)\n",
    "#     #train_dataset, val_dataset = random_split(dataset, [0.7, 0.3], generator1)\n",
    "#     #dataset = CNNDataset(data_csv=P_TRAIN_CSV, target_csv=P_TARGETS_CSV, matrix_dir=ETERNA_PKG_BPP )\n",
    "#     #train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)  \n",
    "#     #val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True) \n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     model = CRNN(img_channel=1, img_width=224, img_height=224)\n",
    "#     print(model)\n",
    "#     #if reload_checkpoint:\n",
    "#     #    crnn.load_state_dict(torch.load(reload_checkpoint, map_location=device))\n",
    "#     model.to(device)\n",
    "\n",
    "#     # ðŸ“ˆ Define the optimizer with learning rate and weight decay\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=5e-4)\n",
    "\n",
    "#     # ðŸš‚ Iterate over epochs\n",
    "#     for epoch in range(epochs):\n",
    "#         train_losses = []\n",
    "#         model.train()\n",
    "        \n",
    "#         # ðŸšž Iterate over batches in the training dataloader\n",
    "#         for batch in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n",
    "#             loss = train_batch(model, batch, optimizer, loss_fn, device)\n",
    "#             train_losses.append(loss.detach().cpu().numpy())\n",
    "#             pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")\n",
    "        \n",
    "#         # ðŸ“Š Print average training loss and MAE for the epoch\n",
    "#         print(f\"Epoch {epoch} train loss: \", np.mean(train_losses))\n",
    "        \n",
    "#         val_losses = []\n",
    "#         model.eval()\n",
    "        \n",
    "#         # ðŸšž Iterate over batches in the validation dataloader\n",
    "#         for batch in (pbar := tqdm(val_dataloader, position=0, leave=True)):\n",
    "#             loss = train_batch(model, batch, optimizer, loss_fn, device)\n",
    "#             optimizer.zero_grad()\n",
    "#             out = model(batch.x, batch.edge_index)\n",
    "#             out = torch.squeeze(out)\n",
    "#             val_losses.append(loss.detach().cpu().numpy())\n",
    "#             pbar.set_description(f\"Validation loss {loss.detach().cpu().numpy():.4f}\")\n",
    "        \n",
    "#         # ðŸ“Š Print average validation loss and MAE for the epoch\n",
    "#         print(f\"Epoch {epoch} val loss: \", np.mean(val_losses))\n",
    "#         print(f\"Epoch {epoch} val mae: \", np.mean(val_maes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets.iloc[0, :-1].values.astype('float32').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = 'DATA/Ribonanza_bpp_files/extra_data/9/8/2/ff4dcd9bf671.txt'\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the data from the file\n",
    "# data = np.loadtxt(test)\n",
    "\n",
    "# # Determine the shape of the array\n",
    "# max_row = int(data[:, 0].max())\n",
    "# max_col = int(data[:, 1].max())\n",
    "\n",
    "# # Create an empty array filled with zeros\n",
    "# filled_array = np.zeros((max_row, max_col))\n",
    "\n",
    "# # Fill the values from the loaded data into the empty array\n",
    "# for row, col, value in data:\n",
    "#     filled_array[int(row) - 1, int(col) - 1] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled_array.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_df = pd.DataFrame(columns=['sequence_id', 'path'])\n",
    "# matrix_df['sequence_id'] = file_paths\n",
    "# matrix_df['path'] = file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(matrix_df['sequence_id'].to_list()) - set(targets['sequence_id'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_df.drop(matrix_df['sequence_id']!=targets['sequence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_path = dataset.matrix_df[dataset.matrix_df['sequence_id'] == '00257e85caac']['path'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torchvision import models\n",
    "from torchmetrics.classification import F1Score, AUROC, Recall, ROC, Accuracy, Precision, Specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channel, img_height, img_width,\n",
    "                 map_to_seq_hidden=64, rnn_hidden=256, leaky_relu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        self.cnn, (output_channel, output_height, output_width) = \\\n",
    "            self._cnn_backbone(img_channel, img_height, img_width, leaky_relu)\n",
    "\n",
    "        self.map_to_seq = nn.Linear(output_channel * output_height, map_to_seq_hidden)\n",
    "\n",
    "        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(2 * rnn_hidden, rnn_hidden, bidirectional=True)\n",
    "\n",
    "        self.dense = nn.Linear(2 * rnn_hidden, 1)\n",
    "  \n",
    "       \n",
    "\n",
    "    def _cnn_backbone(self, img_channel, img_height, img_width, leaky_relu):\n",
    "        assert img_height % 16 == 0\n",
    "        assert img_width % 4 == 0\n",
    "\n",
    "        channels = [img_channel, 64, 128, 256, 256, 512, 512, 512]\n",
    "        kernel_sizes = [3, 3, 3, 3, 3, 3, 2]\n",
    "        strides = [1, 1, 1, 1, 1, 1, 1]\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "            # shape of input: (batch, input_channel, height, width)\n",
    "            input_channel = channels[i]\n",
    "            output_channel = channels[i+1]\n",
    "\n",
    "            cnn.add_module(\n",
    "                f'conv{i}',\n",
    "                nn.Conv2d(input_channel, output_channel, kernel_sizes[i], strides[i], paddings[i])\n",
    "            )\n",
    "\n",
    "            if batch_norm:\n",
    "                cnn.add_module(f'batchnorm{i}', nn.BatchNorm2d(output_channel))\n",
    "\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n",
    "            cnn.add_module(f'relu{i}', relu)\n",
    "\n",
    "        # size of image: (channel, height, width) = (img_channel, img_height, img_width)\n",
    "        conv_relu(0)\n",
    "        cnn.add_module('pooling0', nn.MaxPool2d(kernel_size=(2,1), stride=(2,1)))\n",
    "        # (64, img_height // 2, img_width // 2)\n",
    "\n",
    "        conv_relu(1)\n",
    "        cnn.add_module('pooling1', nn.MaxPool2d(kernel_size=(2,1), stride=(2,1)))\n",
    "        # (128, img_height // 4, img_width // 4)\n",
    "\n",
    "        conv_relu(2)\n",
    "        conv_relu(3)\n",
    "        cnn.add_module(\n",
    "            'pooling2',\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )  # (256, img_height // 8, img_width // 4)\n",
    "\n",
    "        conv_relu(4, batch_norm=True)\n",
    "        conv_relu(5, batch_norm=True)\n",
    "        cnn.add_module(\n",
    "            'pooling3',\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )  # (512, img_height // 16, img_width // 4)\n",
    "\n",
    "        conv_relu(6)  # (512, img_height // 16 - 1, img_width // 4 - 1)\n",
    "\n",
    "        output_channel, output_height, output_width = \\\n",
    "            channels[-1], img_height // 16 - 1, img_width // 4 - 1\n",
    "        print((output_channel, output_height, output_width))\n",
    "        return cnn, (output_channel, output_height, output_width)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # shape of images: (batch, channel, height, width)\n",
    "        conv = self.cnn(images)\n",
    "        \n",
    "        # shape of conv: (batch, channel, height, width)\n",
    "        batch, channel, height, width = conv.size()\n",
    "        conv = conv.view(batch, channel * height, width)\n",
    "        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n",
    "        seq = self.map_to_seq(conv)\n",
    "        \n",
    "        recurrent, _ = self.rnn1(seq)\n",
    "        recurrent, _ = self.rnn2(recurrent)\n",
    "\n",
    "        output = self.dense(recurrent)\n",
    "        # reshape to batch, seq_length\n",
    "        output = output.transpose(0, 1)\n",
    "        return output  # shape: (seq_len, batch, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTrainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        weights=None,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=1e-4,\n",
    "        gamma=2,\n",
    "        num_classes = 223,\n",
    "        num_channels = 1,\n",
    "    ):\n",
    "        \"\"\"Initialize with pretrained weights instead of None if a pretrained model is required.\"\"\"\n",
    "        super().__init__()\n",
    "        self.training_step_losses = []\n",
    "        self.training_step_accuracy = []\n",
    "\n",
    "        self.validation_step_losses = []\n",
    "        self.validation_step_accuracy = []\n",
    "\n",
    "        self.test_step_losses = []\n",
    "\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if num_classes == 2:\n",
    "            self.num_classes = num_classes - 1\n",
    "        elif num_classes > 2:\n",
    "            self.num_classes = num_classes\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.conv_reshape = []\n",
    "        # models ###################\n",
    "        if self.model_name == \"crnn\":\n",
    "            self.model = self._get_model()\n",
    "     \n",
    "        # possible bug with cross entropy and two classes\n",
    "\n",
    "\n",
    "        # metrics ###################\n",
    "        # self.f1 = F1Score(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.auroc = AUROC(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.recall = Recall(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.accuracy = Accuracy(task=, num_classes=self.num_classes)\n",
    "        # self.precision = Precision(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.specifity = Specificity(task=self.classification, num_classes=self.num_classes)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def loss_fn(self, output, target):\n",
    "        # ðŸªŸ Clip the target values to be within the range [0, 1]\n",
    "        clipped_target = torch.clip(target, min=0, max=1)\n",
    "        # ðŸ“‰ Calculate the mean squared error loss\n",
    "        mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n",
    "        return mses\n",
    "\n",
    "        \n",
    "    def _get_model(self):\n",
    "        model = CRNN(img_channel=1, img_width=224, img_height=224)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    # def _metrics(self, y_pred, y_true):\n",
    "    #     f1 = self.f1(y_pred, y_true)\n",
    "    #     auroc = self.auroc(y_pred, y_true)\n",
    "    #     recall = self.recall(y_pred, y_true)\n",
    "    #     precision = self.precision(y_pred, y_true)\n",
    "    #     accuracy = self.accuracy(y_pred, y_true)\n",
    "    #     specifity = self.specifity(y_pred, y_true)\n",
    "    #     return accuracy\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        output = self.model(imgs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "       \n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, y_true = batch\n",
    "        y_pred = self.forward(inputs)\n",
    "        # loss\n",
    "\n",
    "\n",
    "        y_true = y_true.unsqueeze(-1)\n",
    "        loss = self.loss_fn(y_pred, y_true)\n",
    "\n",
    "        # accuracy = self._metrics(y_pred, y_true)\n",
    "\n",
    "        self.training_step_losses.append(loss)\n",
    "        # self.training_step_accuracy.append(accuracy)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True)\n",
    "        # self.log(\"train_accuracy\", accuracy, prog_bar=True, on_step=True)\n",
    "        # self.log(\"train_auroc\", auroc, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_precision\", precision, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_recall\", recall, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_f1\", f1, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_specifity\", specifity, prog_bar=False, on_step=True)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.training_step_losses).mean()\n",
    "        # avg_train_acc = torch.stack(self.training_step_accuracy).mean()\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss_epoch_end\",\n",
    "            avg_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"train_acc_epoch_end\",\n",
    "        #     avg_train_acc,\n",
    "        #     prog_bar=True,\n",
    "        #     on_step=False,\n",
    "        #     on_epoch=True,\n",
    "        # )\n",
    "        return avg_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, y_true = batch\n",
    "        # Forward pass\n",
    "        y_pred = self.forward(inputs)\n",
    "        # loss\n",
    "\n",
    "        y_true = y_true.unsqueeze(-1)\n",
    "        loss = self.loss_fn(y_pred, y_true)\n",
    "\n",
    "        # accuracy = self._metrics(y_pred, y_true)\n",
    "\n",
    "        self.validation_step_losses.append(loss)\n",
    "        # self.validation_step_accuracy.append(accuracy)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=True)\n",
    "        # self.log(\"val_accuracy\", accuracy, prog_bar=True, on_step=True)\n",
    "        # self.log(\"val_auroc\", auroc, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_precision\", precision, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_recall\", recall, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_f1\", f1, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_specifity\", specifity, prog_bar=False, on_step=True)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.validation_step_losses).mean()\n",
    "        # avg_train_acc = torch.stack(self.validation_step_accuracy).mean()\n",
    "\n",
    "        self.log(\n",
    "            \"validation_loss_epoch_end\",\n",
    "            avg_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"validation_acc_epoch_end\",\n",
    "        #     avg_train_acc,\n",
    "        #     prog_bar=True,\n",
    "        #     on_step=False,\n",
    "        #     on_epoch=True,\n",
    "        # )\n",
    "        return avg_loss\n",
    "\n",
    "    # This is for binaryCE loss cuz one dimension \n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs = batch\n",
    "        y_pred = self.forward(inputs)\n",
    "        # if self.classification == 'binary':\n",
    "        #     y_pred = torch.nn.functional.sigmoid(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 13, 55)\n"
     ]
    }
   ],
   "source": [
    "model = CNNTrainer(\"crnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNNTrainer                               [64, 223, 1]              --\n",
      "â”œâ”€CRNN: 1-1                              [64, 223, 1]              --\n",
      "â”‚    â””â”€Sequential: 2-1                   [64, 512, 13, 223]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-1                  [64, 64, 224, 224]        640\n",
      "â”‚    â”‚    â””â”€ReLU: 3-2                    [64, 64, 224, 224]        --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-3               [64, 64, 112, 224]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-4                  [64, 128, 112, 224]       73,856\n",
      "â”‚    â”‚    â””â”€ReLU: 3-5                    [64, 128, 112, 224]       --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-6               [64, 128, 56, 224]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-7                  [64, 256, 56, 224]        295,168\n",
      "â”‚    â”‚    â””â”€ReLU: 3-8                    [64, 256, 56, 224]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-9                  [64, 256, 56, 224]        590,080\n",
      "â”‚    â”‚    â””â”€ReLU: 3-10                   [64, 256, 56, 224]        --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-11              [64, 256, 28, 224]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-12                 [64, 512, 28, 224]        1,180,160\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-13            [64, 512, 28, 224]        1,024\n",
      "â”‚    â”‚    â””â”€ReLU: 3-14                   [64, 512, 28, 224]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-15                 [64, 512, 28, 224]        2,359,808\n",
      "â”‚    â”‚    â””â”€BatchNorm2d: 3-16            [64, 512, 28, 224]        1,024\n",
      "â”‚    â”‚    â””â”€ReLU: 3-17                   [64, 512, 28, 224]        --\n",
      "â”‚    â”‚    â””â”€MaxPool2d: 3-18              [64, 512, 14, 224]        --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-19                 [64, 512, 13, 223]        1,049,088\n",
      "â”‚    â”‚    â””â”€ReLU: 3-20                   [64, 512, 13, 223]        --\n",
      "â”‚    â””â”€Linear: 2-2                       [223, 64, 64]             426,048\n",
      "â”‚    â””â”€LSTM: 2-3                         [223, 64, 512]            659,456\n",
      "â”‚    â””â”€LSTM: 2-4                         [223, 64, 512]            1,576,960\n",
      "â”‚    â””â”€Linear: 2-5                       [223, 64, 1]              513\n",
      "==========================================================================================\n",
      "Total params: 8,213,825\n",
      "Trainable params: 8,213,825\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.TERABYTES): 2.48\n",
      "==========================================================================================\n",
      "Input size (MB): 12.85\n",
      "Forward/backward pass size (MB): 14037.63\n",
      "Params size (MB): 32.86\n",
      "Estimated Total Size (MB): 14083.33\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model, (64, 1, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from config import *\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "                dirpath=CRNN_CHK_PNT,\n",
    "                monitor='val_loss',\n",
    "                save_top_k=1,\n",
    "                filename='best-{epoch}-{val_loss:.2f}'\n",
    "            )\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=CRNN_LOG, name='1epoch_crnn', version=1)\n",
    "\n",
    "pl_trainer = Trainer(max_epochs = 1, accelerator='gpu', devices=1, precision=\"16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | CRNN | 8.2 M \n",
      "-------------------------------\n",
      "8.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.2 M     Total params\n",
      "32.855    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\AppData\\Local\\Temp\\ipykernel_1516\\2532824413.py:57: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 223, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [10:15<00:00,  4.32it/s, v_num=6, train_loss=0.000572]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\AppData\\Local\\Temp\\ipykernel_1516\\2532824413.py:57: UserWarning: Using a target size (torch.Size([59, 1, 1])) that is different to the input size (torch.Size([59, 223, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [12:01<00:00,  3.69it/s, v_num=6, train_loss=0.000572, val_loss_step=0.000176, val_loss_epoch=0.000176, validation_loss_epoch_end=0.000253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\AppData\\Local\\Temp\\ipykernel_1516\\2532824413.py:57: UserWarning: Using a target size (torch.Size([6, 1, 1])) that is different to the input size (torch.Size([6, 223, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [11:39<00:00,  3.80it/s, v_num=6, train_loss=0.00013, val_loss_step=0.000176, val_loss_epoch=0.000176, validation_loss_epoch_end=0.000215, train_loss_epoch_end=0.00188] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2658/2658 [11:39<00:00,  3.80it/s, v_num=6, train_loss=0.00013, val_loss_step=0.000176, val_loss_epoch=0.000176, validation_loss_epoch_end=0.000215, train_loss_epoch_end=0.00188]\n"
     ]
    }
   ],
   "source": [
    "pl_trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 13, 55)\n",
      "Predicting DataLoader 0:   8%|â–Š         | 200/2625 [24:37<4:58:33,  0.14it/s]"
     ]
    }
   ],
   "source": [
    "#Reload your desired checkpoint\n",
    "#model = CNNTrainer.load_from_checkpoint(\"E:\\Studies\\RNA_Reactivity_Prediction\\experiments\\crnn\\checkpoints\\epoch=4-step=10795.ckpt\")\n",
    "y_pred = pl_trainer.predict(model=model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
