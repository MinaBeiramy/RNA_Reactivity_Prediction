{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import *\n",
    "import torch\n",
    "from dataloaders import *\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pandas as pd\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_parquet(P_TRAIN_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>seq_len</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>170</td></tr><tr><td>155</td></tr><tr><td>206</td></tr><tr><td>177</td></tr><tr><td>115</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 1)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ seq_len ‚îÇ\n",
       "‚îÇ ---     ‚îÇ\n",
       "‚îÇ u32     ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 170     ‚îÇ\n",
       "‚îÇ 155     ‚îÇ\n",
       "‚îÇ 206     ‚îÇ\n",
       "‚îÇ 177     ‚îÇ\n",
       "‚îÇ 115     ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select(\"seq_len\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pl.read_parquet(P_TEST_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>seq_len</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>307</td></tr><tr><td>457</td></tr><tr><td>177</td></tr><tr><td>207</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 1)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ seq_len ‚îÇ\n",
       "‚îÇ ---     ‚îÇ\n",
       "‚îÇ u32     ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 307     ‚îÇ\n",
       "‚îÇ 457     ‚îÇ\n",
       "‚îÇ 177     ‚îÇ\n",
       "‚îÇ 207     ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.select(\"seq_len\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(np.loadtxt(\"DATA/Ribonanza_bpp_files/extra_data/0/0/0/0adadce64f93.txt\").T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = torch.zeros(512, 512)\n",
    "init = torch.where(init == 0, float(\"-inf\"), 0).to(torch.float64)\n",
    "max_height = 512\n",
    "max_width = 512\n",
    "\n",
    "x = (data[1] - 1).to(torch.int64)\n",
    "y = (data[0] - 1).to(torch.int64)\n",
    "init[y, x] = data[2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bpp_to_nparray(bpp_file):\n",
    "\n",
    "    # Load the data from the file\n",
    "    data = np.loadtxt(bpp_file)\n",
    "\n",
    "    # Create an empty array filled with zeros\n",
    "    filled_array = np.zeros((512, 512))\n",
    "\n",
    "    # Fill the values from the loaded data into the empty array\n",
    "    for row, col, value in data:\n",
    "        filled_array[int(row) - 1, int(col) - 1] = value\n",
    "    \n",
    "    return filled_array\n",
    "\n",
    "old_init = load_bpp_to_nparray(\"DATA/Ribonanza_bpp_files/extra_data/0/0/0/0adadce64f93.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8014e-05, dtype=torch.float64)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init[6][141]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8014e-05"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_init[6][141]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomended Approach: USE parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dataset = SimpleParquetDataset(P_TRAIN_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUCCGAGGAGACGAACUACCACGAACAGGGGAAACUCUACCCGUGGCGUCUCCGUUUGACGAGUAAGUCCUAAGUCAACAUGCACAGCGCUGGGUUCGCCCAGCGCAAAAGAAACAACAACAACAAC',\n",
       " 'DATA/Ribonanza_bpp_files/extra_data/0/0/0/51e61fbde94d.txt')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_dataset.sequence_df.row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(parquet_dataset, [0.7, 0.3], generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)  \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SimpleParquetDataset(P_TEST_PARQUET, train_test_flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=8, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for y in test_dataloader:\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTTIONAL: use pandas \n",
    "uncomment if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(P_TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = CNNDataset(data_csv=P_TRAIN_CSV, target_csv=P_TARGETS_CSV, matrix_dir=ETERNA_PKG_BPP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.matrix_df[dataset.matrix_df['sequence_id'] == 'eee73c1836bc']['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator1 = torch.Generator().manual_seed(42)\n",
    "# train_dataset, val_dataset = random_split(parquet_dataset, [0.7, 0.3], generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)  \n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = CNNDataset(data_csv=P_TEST_CSV, train_test_flag='test', matrix_dir=ETERNA_PKG_BPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=8, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_dataset.matrix_df[test_dataset.matrix_df['sequence_id'] == 'eee73c1836bc']['path'].values[0]\n",
    "#test_dataset.matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_dataloader:\n",
    "#     print(x.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# import torch.optim as optim\n",
    "# from torch.nn import CTCLoss\n",
    "\n",
    "# from dataloaders import CNNDataset\n",
    "# from config import *\n",
    "\n",
    "\n",
    "# # üìâ Define loss functions for training and evaluation\n",
    "# def loss_fn(output, target):\n",
    "#     # ü™ü Clip the target values to be within the range [0, 1]\n",
    "#     clipped_target = torch.clip(target, min=0, max=1)\n",
    "#     # üìâ Calculate the mean squared error loss\n",
    "#     mses = F.l1_loss(output, clipped_target, reduction='mean')\n",
    "#     return mses\n",
    "\n",
    "# # def mae_fn(output, target):\n",
    "# #     # ü™ü Clip the target values to be within the range [0, 1]\n",
    "# #     clipped_target = torch.clip(target, min=0, max=1)\n",
    "# #     # üìâ Calculate the mean absolute error loss\n",
    "# #     maes = F.l1_loss(output, clipped_target, reduction='mean')\n",
    "# #     return maes\n",
    "\n",
    "# def train_batch(crnn, data, optimizer, criterion, device):\n",
    "#     #crnn.train()\n",
    "\n",
    "#     images, targets= [d.to(device) for d in data]\n",
    "\n",
    "#     logits = crnn(images)\n",
    "#     #log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "#     log_probs = logits\n",
    "\n",
    "#     #atch_size = images.size(0)\n",
    "#     #input_lengths = torch.LongTensor([logits.size(0)] * batch_size)\n",
    "#     #target_lengths = torch.flatten(input_lengths)\n",
    "\n",
    "#     loss = criterion(log_probs, targets)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(crnn.parameters(), 5) # gradient clipping with 5\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     epochs = 1\n",
    "#     #dataset = CNNDataset(data_csv=P_TRAIN_CSV, target_csv=P_TARGETS_CSV, matrix_dir=ETERNA_PKG_BPP )\n",
    "#     generator1 = torch.Generator().manual_seed(42)\n",
    "#     #train_dataset, val_dataset = random_split(dataset, [0.7, 0.3], generator1)\n",
    "#     #dataset = CNNDataset(data_csv=P_TRAIN_CSV, target_csv=P_TARGETS_CSV, matrix_dir=ETERNA_PKG_BPP )\n",
    "#     #train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)  \n",
    "#     #val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True) \n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     model = CRNN(img_channel=1, img_width=224, img_height=224)\n",
    "#     print(model)\n",
    "#     #if reload_checkpoint:\n",
    "#     #    crnn.load_state_dict(torch.load(reload_checkpoint, map_location=device))\n",
    "#     model.to(device)\n",
    "\n",
    "#     # üìà Define the optimizer with learning rate and weight decay\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=5e-4)\n",
    "\n",
    "#     # üöÇ Iterate over epochs\n",
    "#     for epoch in range(epochs):\n",
    "#         train_losses = []\n",
    "#         model.train()\n",
    "        \n",
    "#         # üöû Iterate over batches in the training dataloader\n",
    "#         for batch in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n",
    "#             loss = train_batch(model, batch, optimizer, loss_fn, device)\n",
    "#             train_losses.append(loss.detach().cpu().numpy())\n",
    "#             pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")\n",
    "        \n",
    "#         # üìä Print average training loss and MAE for the epoch\n",
    "#         print(f\"Epoch {epoch} train loss: \", np.mean(train_losses))\n",
    "        \n",
    "#         val_losses = []\n",
    "#         model.eval()\n",
    "        \n",
    "#         # üöû Iterate over batches in the validation dataloader\n",
    "#         for batch in (pbar := tqdm(val_dataloader, position=0, leave=True)):\n",
    "#             loss = train_batch(model, batch, optimizer, loss_fn, device)\n",
    "#             optimizer.zero_grad()\n",
    "#             out = model(batch.x, batch.edge_index)\n",
    "#             out = torch.squeeze(out)\n",
    "#             val_losses.append(loss.detach().cpu().numpy())\n",
    "#             pbar.set_description(f\"Validation loss {loss.detach().cpu().numpy():.4f}\")\n",
    "        \n",
    "#         # üìä Print average validation loss and MAE for the epoch\n",
    "#         print(f\"Epoch {epoch} val loss: \", np.mean(val_losses))\n",
    "#         print(f\"Epoch {epoch} val mae: \", np.mean(val_maes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets.iloc[0, :-1].values.astype('float32').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = 'DATA/Ribonanza_bpp_files/extra_data/9/8/2/ff4dcd9bf671.txt'\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the data from the file\n",
    "# data = np.loadtxt(test)\n",
    "\n",
    "# # Determine the shape of the array\n",
    "# max_row = int(data[:, 0].max())\n",
    "# max_col = int(data[:, 1].max())\n",
    "\n",
    "# # Create an empty array filled with zeros\n",
    "# filled_array = np.zeros((max_row, max_col))\n",
    "\n",
    "# # Fill the values from the loaded data into the empty array\n",
    "# for row, col, value in data:\n",
    "#     filled_array[int(row) - 1, int(col) - 1] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled_array.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_df = pd.DataFrame(columns=['sequence_id', 'path'])\n",
    "# matrix_df['sequence_id'] = file_paths\n",
    "# matrix_df['path'] = file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(matrix_df['sequence_id'].to_list()) - set(targets['sequence_id'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_df.drop(matrix_df['sequence_id']!=targets['sequence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_path = dataset.matrix_df[dataset.matrix_df['sequence_id'] == '00257e85caac']['path'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from torchvision import models\n",
    "from torchmetrics.classification import F1Score, AUROC, Recall, ROC, Accuracy, Precision, Specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channel, img_height, img_width,\n",
    "                 map_to_seq_hidden=64, rnn_hidden=256, leaky_relu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        self.cnn, (output_channel, output_height, output_width) = \\\n",
    "            self._cnn_backbone(img_channel, img_height, img_width, leaky_relu)\n",
    "\n",
    "        self.map_to_seq = nn.Linear(output_channel * output_height, map_to_seq_hidden)\n",
    "\n",
    "        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(2 * rnn_hidden, rnn_hidden, bidirectional=True)\n",
    "\n",
    "        self.dense = nn.Linear(2 * rnn_hidden, 1)\n",
    "  \n",
    "       \n",
    "\n",
    "    def _cnn_backbone(self, img_channel, img_height, img_width, leaky_relu):\n",
    "        assert img_height % 16 == 0\n",
    "        assert img_width % 4 == 0\n",
    "\n",
    "        channels = [img_channel, 64, 128, 256, 256, 512, 512, 512]\n",
    "        kernel_sizes = [3, 3, 3, 3, 3, 3, 3]\n",
    "        strides = [1, 1, 1, 1, 1, 1, 1]\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "            # shape of input: (batch, input_channel, height, width)\n",
    "            input_channel = channels[i]\n",
    "            output_channel = channels[i+1]\n",
    "\n",
    "            cnn.add_module(\n",
    "                f'conv{i}',\n",
    "                nn.Conv2d(input_channel, output_channel, kernel_sizes[i], strides[i], paddings[i])\n",
    "            )\n",
    "\n",
    "            if batch_norm:\n",
    "                cnn.add_module(f'batchnorm{i}', nn.BatchNorm2d(output_channel))\n",
    "\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n",
    "            cnn.add_module(f'relu{i}', relu)\n",
    "\n",
    "        # size of image: (channel, height, width) = (img_channel, img_height, img_width)\n",
    "        conv_relu(0)\n",
    "        cnn.add_module('pooling0', nn.MaxPool2d(kernel_size=(2,1), stride=(2,1)))\n",
    "        # (64, img_height // 2, img_width // 2)\n",
    "\n",
    "        conv_relu(1)\n",
    "        cnn.add_module('pooling1', nn.MaxPool2d(kernel_size=(2,1), stride=(2,1)))\n",
    "        # (128, img_height // 4, img_width // 4)\n",
    "\n",
    "        conv_relu(2)\n",
    "        conv_relu(3)\n",
    "        cnn.add_module(\n",
    "            'pooling2',\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )  # (256, img_height // 8, img_width // 4)\n",
    "\n",
    "        conv_relu(4, batch_norm=True)\n",
    "        conv_relu(5, batch_norm=True)\n",
    "        cnn.add_module(\n",
    "            'pooling3',\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )  # (512, img_height // 16, img_width // 4)\n",
    "\n",
    "        conv_relu(6)  # (512, img_height // 16 - 1, img_width // 4 - 1)\n",
    "\n",
    "        output_channel, output_height, output_width = \\\n",
    "            channels[-1], img_height // 16 , img_width // 4 \n",
    "        print((output_channel, output_height, output_width))\n",
    "        return cnn, (output_channel, output_height, output_width)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # shape of images: (batch, channel, height, width)\n",
    "        conv = self.cnn(images)\n",
    "        \n",
    "        # shape of conv: (batch, channel, height, width)\n",
    "        batch, channel, height, width = conv.size()\n",
    "        conv = conv.view(batch, channel * height, width)\n",
    "        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n",
    "    \n",
    "        seq = self.map_to_seq(conv)\n",
    "        \n",
    "        recurrent, _ = self.rnn1(seq)\n",
    "        recurrent, _ = self.rnn2(recurrent)\n",
    "\n",
    "        output = self.dense(recurrent)\n",
    "        # reshape to batch, seq_length\n",
    "        output = output.transpose(0, 1)\n",
    "        return output  # shape: (seq_len, batch, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTrainer(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        weights=None,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=1e-4,\n",
    "        gamma=2,\n",
    "        num_classes = 511,\n",
    "        num_channels = 1,\n",
    "    ):\n",
    "        \"\"\"Initialize with pretrained weights instead of None if a pretrained model is required.\"\"\"\n",
    "        super().__init__()\n",
    "        self.training_step_losses = []\n",
    "        self.training_step_accuracy = []\n",
    "\n",
    "        self.validation_step_losses = []\n",
    "        self.validation_step_accuracy = []\n",
    "\n",
    "        self.test_step_losses = []\n",
    "\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if num_classes == 2:\n",
    "            self.num_classes = num_classes - 1\n",
    "        elif num_classes > 2:\n",
    "            self.num_classes = num_classes\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.conv_reshape = []\n",
    "        # models ###################\n",
    "        if self.model_name == \"crnn\":\n",
    "            self.model = self._get_model()\n",
    "     \n",
    "        # possible bug with cross entropy and two classes\n",
    "\n",
    "\n",
    "        # metrics ###################\n",
    "        # self.f1 = F1Score(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.auroc = AUROC(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.recall = Recall(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.accuracy = Accuracy(task=, num_classes=self.num_classes)\n",
    "        # self.precision = Precision(task=self.classification, num_classes=self.num_classes)\n",
    "        # self.specifity = Specificity(task=self.classification, num_classes=self.num_classes)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def loss_fn(self, output, target):\n",
    "        # ü™ü Clip the target values to be within the range [0, 1]\n",
    "        clipped_target = torch.clip(target, min=0, max=1)\n",
    "        # üìâ Calculate the mean squared error loss\n",
    "        mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n",
    "        return mses\n",
    "\n",
    "        \n",
    "    def _get_model(self):\n",
    "        model = CRNN(img_channel=1, img_width=512, img_height=512)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    # def _metrics(self, y_pred, y_true):\n",
    "    #     f1 = self.f1(y_pred, y_true)\n",
    "    #     auroc = self.auroc(y_pred, y_true)\n",
    "    #     recall = self.recall(y_pred, y_true)\n",
    "    #     precision = self.precision(y_pred, y_true)\n",
    "    #     accuracy = self.accuracy(y_pred, y_true)\n",
    "    #     specifity = self.specifity(y_pred, y_true)\n",
    "    #     return accuracy\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        output = self.model(imgs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "       \n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, y_true = batch\n",
    "        y_pred = self.forward(inputs)\n",
    "        # loss\n",
    "\n",
    "\n",
    "        y_true = y_true.unsqueeze(-1)\n",
    "        loss = self.loss_fn(y_pred, y_true)\n",
    "\n",
    "        # accuracy = self._metrics(y_pred, y_true)\n",
    "\n",
    "        self.training_step_losses.append(loss)\n",
    "        # self.training_step_accuracy.append(accuracy)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True)\n",
    "        # self.log(\"train_accuracy\", accuracy, prog_bar=True, on_step=True)\n",
    "        # self.log(\"train_auroc\", auroc, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_precision\", precision, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_recall\", recall, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_f1\", f1, prog_bar=False, on_step=True)\n",
    "        # self.log(\"train_specifity\", specifity, prog_bar=False, on_step=True)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.training_step_losses).mean()\n",
    "        # avg_train_acc = torch.stack(self.training_step_accuracy).mean()\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss_epoch_end\",\n",
    "            avg_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"train_acc_epoch_end\",\n",
    "        #     avg_train_acc,\n",
    "        #     prog_bar=True,\n",
    "        #     on_step=False,\n",
    "        #     on_epoch=True,\n",
    "        # )\n",
    "        return avg_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, y_true = batch\n",
    "        # Forward pass\n",
    "        y_pred = self.forward(inputs)\n",
    "        # loss\n",
    "\n",
    "        y_true = y_true.unsqueeze(-1)\n",
    "        loss = self.loss_fn(y_pred, y_true)\n",
    "\n",
    "        # accuracy = self._metrics(y_pred, y_true)\n",
    "\n",
    "        self.validation_step_losses.append(loss)\n",
    "        # self.validation_step_accuracy.append(accuracy)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=True)\n",
    "        # self.log(\"val_accuracy\", accuracy, prog_bar=True, on_step=True)\n",
    "        # self.log(\"val_auroc\", auroc, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_precision\", precision, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_recall\", recall, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_f1\", f1, prog_bar=False, on_step=True)\n",
    "        # self.log(\"val_specifity\", specifity, prog_bar=False, on_step=True)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.validation_step_losses).mean()\n",
    "        # avg_train_acc = torch.stack(self.validation_step_accuracy).mean()\n",
    "\n",
    "        self.log(\n",
    "            \"validation_loss_epoch_end\",\n",
    "            avg_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        # self.log(\n",
    "        #     \"validation_acc_epoch_end\",\n",
    "        #     avg_train_acc,\n",
    "        #     prog_bar=True,\n",
    "        #     on_step=False,\n",
    "        #     on_epoch=True,\n",
    "        # )\n",
    "        return avg_loss\n",
    "\n",
    "    # This is for binaryCE loss cuz one dimension \n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs = batch\n",
    "        y_pred = self.forward(inputs)\n",
    "        # if self.classification == 'binary':\n",
    "        #     y_pred = torch.nn.functional.sigmoid(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "model = CNNTrainer(\"crnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNNTrainer                               [64, 512, 1]              --\n",
      "‚îú‚îÄCRNN: 1-1                              [64, 512, 1]              --\n",
      "‚îÇ    ‚îî‚îÄSequential: 2-1                   [64, 512, 32, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                  [64, 64, 512, 512]        640\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-2                    [64, 64, 512, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-3               [64, 64, 256, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                  [64, 128, 256, 512]       73,856\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-5                    [64, 128, 256, 512]       --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-6               [64, 128, 128, 512]       --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-7                  [64, 256, 128, 512]       295,168\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-8                    [64, 256, 128, 512]       --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-9                  [64, 256, 128, 512]       590,080\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-10                   [64, 256, 128, 512]       --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-11              [64, 256, 64, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-12                 [64, 512, 64, 512]        1,180,160\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-13            [64, 512, 64, 512]        1,024\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-14                   [64, 512, 64, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-15                 [64, 512, 64, 512]        2,359,808\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-16            [64, 512, 64, 512]        1,024\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-17                   [64, 512, 64, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-18              [64, 512, 32, 512]        --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-19                 [64, 512, 32, 512]        2,359,808\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-20                   [64, 512, 32, 512]        --\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-2                       [512, 64, 64]             1,048,640\n",
      "‚îÇ    ‚îî‚îÄLSTM: 2-3                         [512, 64, 512]            659,456\n",
      "‚îÇ    ‚îî‚îÄLSTM: 2-4                         [512, 64, 512]            1,576,960\n",
      "‚îÇ    ‚îî‚îÄLinear: 2-5                       [512, 64, 1]              513\n",
      "==========================================================================================\n",
      "Total params: 10,147,137\n",
      "Trainable params: 10,147,137\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.TERABYTES): 14.32\n",
      "==========================================================================================\n",
      "Input size (MB): 67.11\n",
      "Forward/backward pass size (MB): 73299.92\n",
      "Params size (MB): 40.59\n",
      "Estimated Total Size (MB): 73407.62\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model, (64, 1, 512, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from config import *\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "                dirpath=CRNN_CHK_PNT,\n",
    "                monitor='val_loss',\n",
    "                save_top_k=1,\n",
    "                filename='best-{epoch}-{val_loss:.2f}'\n",
    "            )\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=CRNN_LOG, name='1epoch_crnn', version=1)\n",
    "\n",
    "pl_trainer = Trainer(max_epochs = 1, accelerator='gpu', devices=1, precision=\"16-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | CRNN | 8.2 M \n",
      "-------------------------------\n",
      "8.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.2 M     Total params\n",
      "32.855    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\AppData\\Local\\Temp\\ipykernel_1516\\2532824413.py:57: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 223, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2658/2658 [10:15<00:00,  4.32it/s, v_num=6, train_loss=0.000572]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\AppData\\Local\\Temp\\ipykernel_1516\\2532824413.py:57: UserWarning: Using a target size (torch.Size([59, 1, 1])) that is different to the input size (torch.Size([59, 223, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2658/2658 [12:01<00:00,  3.69it/s, v_num=6, train_loss=0.000572, val_loss_step=0.000176, val_loss_epoch=0.000176, validation_loss_epoch_end=0.000253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raj\\AppData\\Local\\Temp\\ipykernel_1516\\2532824413.py:57: UserWarning: Using a target size (torch.Size([6, 1, 1])) that is different to the input size (torch.Size([6, 223, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mses = torch.nn.functional.l1_loss(output, clipped_target, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2658/2658 [11:39<00:00,  3.80it/s, v_num=6, train_loss=0.00013, val_loss_step=0.000176, val_loss_epoch=0.000176, validation_loss_epoch_end=0.000215, train_loss_epoch_end=0.00188] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2658/2658 [11:39<00:00,  3.80it/s, v_num=6, train_loss=0.00013, val_loss_step=0.000176, val_loss_epoch=0.000176, validation_loss_epoch_end=0.000215, train_loss_epoch_end=0.00188]\n"
     ]
    }
   ],
   "source": [
    "pl_trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 13, 55)\n",
      "Predicting DataLoader 0:   8%|‚ñä         | 200/2625 [24:37<4:58:33,  0.14it/s]"
     ]
    }
   ],
   "source": [
    "#Reload your desired checkpoint\n",
    "#model = CNNTrainer.load_from_checkpoint(\"E:\\Studies\\RNA_Reactivity_Prediction\\experiments\\crnn\\checkpoints\\epoch=4-step=10795.ckpt\")\n",
    "y_pred = pl_trainer.predict(model=model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
